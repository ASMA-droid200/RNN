{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMWtBKI3CFCbJtCzfo+eKX3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ASMA-droid200/RNN/blob/main/21_03_24.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "train ner model iin my custom dataset"
      ],
      "metadata": {
        "id": "wr2lztXP49Qx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy opencv-python pytesseract"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1D5o_Xc5Nz0",
        "outputId": "2e074d7e-01b0-43b5-a0a9-7d5d88a48655"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.4)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.10-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.6.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.25.2)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (9.4.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import json\n",
        "import pytesseract\n",
        "import cv2\n"
      ],
      "metadata": {
        "id": "n3QRrWhx5Dpx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hmRLmt5y46cF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "outputId": "60be1fe7-1aff-4a74-e042-50f3a422da6b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Load the annotated data\\nwith open(\"annotations.json\", \"r\") as f:\\n    annotated_data = json.load(f)\\n\\n# Initialize a blank spaCy model with NER component\\nnlp = spacy.blank(\"en\")\\nner = nlp.add_pipe(\"ner\")\\n\\n# Add labels to the NER component\\nfor label in annotated_data[\"classes\"]:\\n    ner.add_label(label)\\n\\n# Prepare training data\\ntrain_data = []\\nfor annotation in annotated_data[\"annotations\"]:\\n    text, annotations = annotation\\n    if annotations is not None:  # Check if annotations exist\\n        train_data.append((text, annotations))\\n\\n# Train the NER model\\nn_iter = 10\\nfor _ in range(n_iter):\\n    for text, annotations in train_data:\\n        doc = nlp.make_doc(text)\\n        example = spacy.training.Example.from_dict(doc, annotations)\\n        nlp.update([example])\\n\\n# Save the trained model to disk\\nnlp.to_disk(\"trained_ner_model\")\\n\\n# Test the trained model on new invoice images\\ndef extract_text_from_image(image_path):\\n    # Read the image using OpenCV\\n    image = cv2.imread(image_path)\\n    # Convert the image to grayscale\\n    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\\n    # Thresholding to remove noise\\n    _, thresholded_image = cv2.threshold(gray_image, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\\n    # Perform OCR using pytesseract\\n    text = pytesseract.image_to_string(thresholded_image, lang=\\'eng\\', config=\\'--psm 6\\')\\n    return text\\n\\ndef extract_entities_from_text(text):\\n    doc = nlp(text)\\n    entities = {}\\n    for ent in doc.ents:\\n        entities[ent.label_] = ent.text\\n    return entities\\n\\n# Example usage\\nimage_path = \"/content/test_facture.jpg\"  # Replace with the path to your new invoice image file\\ntext = extract_text_from_image(image_path)\\nentities = extract_entities_from_text(text)\\nprint(\"Extracted entities:\", entities)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "'''\n",
        "# Load the annotated data\n",
        "with open(\"annotations.json\", \"r\") as f:\n",
        "    annotated_data = json.load(f)\n",
        "\n",
        "# Initialize a blank spaCy model with NER component\n",
        "nlp = spacy.blank(\"en\")\n",
        "ner = nlp.add_pipe(\"ner\")\n",
        "\n",
        "# Add labels to the NER component\n",
        "for label in annotated_data[\"classes\"]:\n",
        "    ner.add_label(label)\n",
        "\n",
        "# Prepare training data\n",
        "train_data = []\n",
        "for annotation in annotated_data[\"annotations\"]:\n",
        "    text, annotations = annotation\n",
        "    if annotations is not None:  # Check if annotations exist\n",
        "        train_data.append((text, annotations))\n",
        "\n",
        "# Train the NER model\n",
        "n_iter = 10\n",
        "for _ in range(n_iter):\n",
        "    for text, annotations in train_data:\n",
        "        doc = nlp.make_doc(text)\n",
        "        example = spacy.training.Example.from_dict(doc, annotations)\n",
        "        nlp.update([example])\n",
        "\n",
        "# Save the trained model to disk\n",
        "nlp.to_disk(\"trained_ner_model\")\n",
        "\n",
        "# Test the trained model on new invoice images\n",
        "def extract_text_from_image(image_path):\n",
        "    # Read the image using OpenCV\n",
        "    image = cv2.imread(image_path)\n",
        "    # Convert the image to grayscale\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    # Thresholding to remove noise\n",
        "    _, thresholded_image = cv2.threshold(gray_image, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
        "    # Perform OCR using pytesseract\n",
        "    text = pytesseract.image_to_string(thresholded_image, lang='eng', config='--psm 6')\n",
        "    return text\n",
        "\n",
        "def extract_entities_from_text(text):\n",
        "    doc = nlp(text)\n",
        "    entities = {}\n",
        "    for ent in doc.ents:\n",
        "        entities[ent.label_] = ent.text\n",
        "    return entities\n",
        "\n",
        "# Example usage\n",
        "image_path = \"/content/test_facture.jpg\"  # Replace with the path to your new invoice image file\n",
        "text = extract_text_from_image(image_path)\n",
        "entities = extract_entities_from_text(text)\n",
        "print(\"Extracted entities:\", entities)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install tesseract-ocr\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykIhvtek7wtf",
        "outputId": "2a4605d0-f8a8-44db-8d7c-4cc58f53c15c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 3 newly installed, 0 to remove and 39 not upgraded.\n",
            "Need to get 4,816 kB of archives.\n",
            "After this operation, 15.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-eng all 1:4.00~git30-7274cfa-1.1 [1,591 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-osd all 1:4.00~git30-7274cfa-1.1 [2,990 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr amd64 4.1.1-2.1build1 [236 kB]\n",
            "Fetched 4,816 kB in 1s (3,213 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 121753 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "import spacy\n",
        "import json\n",
        "import cv2\n",
        "import pytesseract\n",
        "from spacy.training import Example\n",
        "\n",
        "# Load the annotated data\n",
        "with open(\"annotations.json\", \"r\") as f:\n",
        "    annotated_data = json.load(f)\n",
        "\n",
        "# Initialize a blank spaCy model with NER component\n",
        "nlp = spacy.blank(\"en\")\n",
        "ner = nlp.add_pipe(\"ner\")\n",
        "\n",
        "# Add labels to the NER component\n",
        "for label in annotated_data[\"classes\"]:\n",
        "    ner.add_label(label)\n",
        "\n",
        "# Prepare training data\n",
        "train_data = []\n",
        "for annotation in annotated_data[\"annotations\"]:\n",
        "    if annotation is not None:  # Check if annotation is not None\n",
        "        text, annotations = annotation\n",
        "        if annotations:  # Check if annotations exist\n",
        "            train_data.append((text, annotations))\n",
        "\n",
        "# Convert annotations to Examples\n",
        "examples = []\n",
        "for text, annotations in train_data:\n",
        "    doc = nlp.make_doc(text)\n",
        "    example = Example.from_dict(doc, annotations)\n",
        "    examples.append(example)\n",
        "\n",
        "# Update the model with the training data\n",
        "optimizer = nlp.begin_training()\n",
        "for _ in range(10):  # Update for 10 iterations\n",
        "    losses = {}\n",
        "    for example in examples:\n",
        "        nlp.update([example], sgd=optimizer, losses=losses)\n",
        "    print(\"Losses:\", losses)\n",
        "\n",
        "# Save the trained model to disk\n",
        "nlp.to_disk(\"trained_ner_model\")\n",
        "\n",
        "# Test the trained model on new invoice images\n",
        "def extract_text_from_image(image_path):\n",
        "    # Read the image using OpenCV\n",
        "    image = cv2.imread(image_path)\n",
        "    # Convert the image to grayscale\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    # Thresholding to remove noise\n",
        "    _, thresholded_image = cv2.threshold(gray_image, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
        "    # Perform OCR using pytesseract\n",
        "    text = pytesseract.image_to_string(thresholded_image, lang='eng', config='--psm 6')\n",
        "    return text\n",
        "\n",
        "def extract_entities_from_text(text):\n",
        "    doc = nlp(text)\n",
        "    entities = {}\n",
        "    for ent in doc.ents:\n",
        "        entities[ent.label_] = ent.text\n",
        "    return entities\n",
        "\n",
        "# Example usage\n",
        "image_path = \"/content/Invoice_1_page-0001.jpg\"  # Replace with the path to your new invoice image file\n",
        "text = extract_text_from_image(image_path)\n",
        "entities = extract_entities_from_text(text)\n",
        "print(\"Extracted entities:\", entities)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "EZL4DoOp618f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "outputId": "2a8b5d05-1385-4413-b18d-00bf20344c61"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nimport spacy\\nimport json\\nimport cv2\\nimport pytesseract\\nfrom spacy.training import Example\\n\\n# Load the annotated data\\nwith open(\"annotations.json\", \"r\") as f:\\n    annotated_data = json.load(f)\\n\\n# Initialize a blank spaCy model with NER component\\nnlp = spacy.blank(\"en\")\\nner = nlp.add_pipe(\"ner\")\\n\\n# Add labels to the NER component\\nfor label in annotated_data[\"classes\"]:\\n    ner.add_label(label)\\n\\n# Prepare training data\\ntrain_data = []\\nfor annotation in annotated_data[\"annotations\"]:\\n    if annotation is not None:  # Check if annotation is not None\\n        text, annotations = annotation\\n        if annotations:  # Check if annotations exist\\n            train_data.append((text, annotations))\\n\\n# Convert annotations to Examples\\nexamples = []\\nfor text, annotations in train_data:\\n    doc = nlp.make_doc(text)\\n    example = Example.from_dict(doc, annotations)\\n    examples.append(example)\\n\\n# Update the model with the training data\\noptimizer = nlp.begin_training()\\nfor _ in range(10):  # Update for 10 iterations\\n    losses = {}\\n    for example in examples:\\n        nlp.update([example], sgd=optimizer, losses=losses)\\n    print(\"Losses:\", losses)\\n\\n# Save the trained model to disk\\nnlp.to_disk(\"trained_ner_model\")\\n\\n# Test the trained model on new invoice images\\ndef extract_text_from_image(image_path):\\n    # Read the image using OpenCV\\n    image = cv2.imread(image_path)\\n    # Convert the image to grayscale\\n    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\\n    # Thresholding to remove noise\\n    _, thresholded_image = cv2.threshold(gray_image, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\\n    # Perform OCR using pytesseract\\n    text = pytesseract.image_to_string(thresholded_image, lang=\\'eng\\', config=\\'--psm 6\\')\\n    return text\\n\\ndef extract_entities_from_text(text):\\n    doc = nlp(text)\\n    entities = {}\\n    for ent in doc.ents:\\n        entities[ent.label_] = ent.text\\n    return entities\\n\\n# Example usage\\nimage_path = \"/content/Invoice_1_page-0001.jpg\"  # Replace with the path to your new invoice image file\\ntext = extract_text_from_image(image_path)\\nentities = extract_entities_from_text(text)\\nprint(\"Extracted entities:\", entities)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "import spacy\n",
        "import json\n",
        "import cv2\n",
        "import pytesseract\n",
        "from spacy.training import Example\n",
        "\n",
        "# Load the annotated data\n",
        "with open(\"annotations.json\", \"r\") as f:\n",
        "    annotated_data = json.load(f)\n",
        "\n",
        "# Initialize a blank spaCy model with NER component\n",
        "nlp = spacy.blank(\"en\")\n",
        "ner = nlp.add_pipe(\"ner\")\n",
        "\n",
        "# Add labels to the NER component\n",
        "for label in annotated_data[\"classes\"]:\n",
        "    ner.add_label(label)\n",
        "\n",
        "# Prepare training data\n",
        "train_data = []\n",
        "for annotation in annotated_data[\"annotations\"]:\n",
        "    if annotation is not None:  # Check if annotation is not None\n",
        "        text, annotations = annotation\n",
        "        if annotations:  # Check if annotations exist\n",
        "            train_data.append((text, annotations))\n",
        "\n",
        "# Convert annotations to Examples\n",
        "examples = []\n",
        "for text, annotations in train_data:\n",
        "    doc = nlp.make_doc(text)\n",
        "    example = Example.from_dict(doc, annotations)\n",
        "    examples.append(example)\n",
        "\n",
        "# Update the model with the training data\n",
        "optimizer = nlp.begin_training()\n",
        "for _ in range(10):  # Update for 10 iterations\n",
        "    losses = {}\n",
        "    for example in examples:\n",
        "        nlp.update([example], sgd=optimizer, losses=losses)\n",
        "    print(\"Losses:\", losses)\n",
        "\n",
        "# Save the trained model to disk\n",
        "nlp.to_disk(\"trained_ner_model\")\n",
        "\n",
        "# Test the trained model on new invoice images\n",
        "def extract_text_from_image(image_path):\n",
        "    # Read the image using OpenCV\n",
        "    image = cv2.imread(image_path)\n",
        "    # Convert the image to grayscale\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    # Thresholding to remove noise\n",
        "    _, thresholded_image = cv2.threshold(gray_image, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
        "    # Perform OCR using pytesseract\n",
        "    text = pytesseract.image_to_string(thresholded_image, lang='eng', config='--psm 6')\n",
        "    return text\n",
        "\n",
        "def extract_entities_from_text(text):\n",
        "    doc = nlp(text)\n",
        "    entities = {}\n",
        "    for ent in doc.ents:\n",
        "        entities[ent.label_] = ent.text\n",
        "    return entities\n",
        "\n",
        "def save_entities_to_json(entities, output_file):\n",
        "    with open(output_file, \"w\") as json_file:\n",
        "        json.dump(entities, json_file, indent=4)\n",
        "\n",
        "# Example usage\n",
        "image_path = \"/content/test_facture.jpg\"  # Replace with the path to your new invoice image file\n",
        "text = extract_text_from_image(image_path)\n",
        "entities = extract_entities_from_text(text)\n",
        "print(\"Extracted entities:\", entities)\n",
        "'''\n",
        "#The extracted entities will be automatically structured based on the predefined keys specified in the entities dictionary.\n",
        "def extract_entities_from_text(text):\n",
        "    doc = nlp(text)\n",
        "    entities = {\n",
        "        \"FACTURE_NUMBER\": \"\",\n",
        "        \"COMPANY\": \"\",\n",
        "        \"DATE\": \"\",\n",
        "        \"ADDRESS\": \"\",\n",
        "        \"QUANTITY\": 0,\n",
        "        \"PRICE\": 0.0\n",
        "    }\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ == \"FACTURE_NUM\":\n",
        "            entities[\"FACTURE_NUMBER\"] = ent.text\n",
        "        elif ent.label_ == \"COMPANY\":\n",
        "            entities[\"COMPANY\"] = ent.text\n",
        "        elif ent.label_ == \"DATE\":\n",
        "            entities[\"DATE\"] = ent.text\n",
        "        elif ent.label_ == \"ADDRESS\":\n",
        "            entities[\"ADDRESS\"] = ent.text\n",
        "        elif ent.label_ == \"QUANTITY\":\n",
        "            entities[\"QUANTITY\"] = int(ent.text)\n",
        "        elif ent.label_ == \"PRICE\":\n",
        "            entities[\"PRICE\"] = float(ent.text)\n",
        "    return entities\n",
        "\n",
        "'''\n",
        "\n",
        "# Save extracted entities to JSON file\n",
        "output_file = \"extracted_entities.json\"\n",
        "save_entities_to_json(entities, output_file)\n",
        "print(\"Extracted entities saved to:\", output_file)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Jw2ChSG1_YVg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "outputId": "a70580d1-1b11-44f9-d062-abbc54f9b806"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nimport spacy\\nimport json\\nimport cv2\\nimport pytesseract\\nfrom spacy.training import Example\\n\\n# Load the annotated data\\nwith open(\"annotations.json\", \"r\") as f:\\n    annotated_data = json.load(f)\\n\\n# Initialize a blank spaCy model with NER component\\nnlp = spacy.blank(\"en\")\\nner = nlp.add_pipe(\"ner\")\\n\\n# Add labels to the NER component\\nfor label in annotated_data[\"classes\"]:\\n    ner.add_label(label)\\n\\n# Prepare training data\\ntrain_data = []\\nfor annotation in annotated_data[\"annotations\"]:\\n    if annotation is not None:  # Check if annotation is not None\\n        text, annotations = annotation\\n        if annotations:  # Check if annotations exist\\n            train_data.append((text, annotations))\\n\\n# Convert annotations to Examples\\nexamples = []\\nfor text, annotations in train_data:\\n    doc = nlp.make_doc(text)\\n    example = Example.from_dict(doc, annotations)\\n    examples.append(example)\\n\\n# Update the model with the training data\\noptimizer = nlp.begin_training()\\nfor _ in range(10):  # Update for 10 iterations\\n    losses = {}\\n    for example in examples:\\n        nlp.update([example], sgd=optimizer, losses=losses)\\n    print(\"Losses:\", losses)\\n\\n# Save the trained model to disk\\nnlp.to_disk(\"trained_ner_model\")\\n\\n# Test the trained model on new invoice images\\ndef extract_text_from_image(image_path):\\n    # Read the image using OpenCV\\n    image = cv2.imread(image_path)\\n    # Convert the image to grayscale\\n    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\\n    # Thresholding to remove noise\\n    _, thresholded_image = cv2.threshold(gray_image, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\\n    # Perform OCR using pytesseract\\n    text = pytesseract.image_to_string(thresholded_image, lang=\\'eng\\', config=\\'--psm 6\\')\\n    return text\\n\\ndef extract_entities_from_text(text):\\n    doc = nlp(text)\\n    entities = {}\\n    for ent in doc.ents:\\n        entities[ent.label_] = ent.text\\n    return entities\\n\\ndef save_entities_to_json(entities, output_file):\\n    with open(output_file, \"w\") as json_file:\\n        json.dump(entities, json_file, indent=4)\\n\\n# Example usage\\nimage_path = \"/content/test_facture.jpg\"  # Replace with the path to your new invoice image file\\ntext = extract_text_from_image(image_path)\\nentities = extract_entities_from_text(text)\\nprint(\"Extracted entities:\", entities)\\n\\'\\'\\'\\n#The extracted entities will be automatically structured based on the predefined keys specified in the entities dictionary.\\ndef extract_entities_from_text(text):\\n    doc = nlp(text)\\n    entities = {\\n        \"FACTURE_NUMBER\": \"\",\\n        \"COMPANY\": \"\",\\n        \"DATE\": \"\",\\n        \"ADDRESS\": \"\",\\n        \"QUANTITY\": 0,\\n        \"PRICE\": 0.0\\n    }\\n    for ent in doc.ents:\\n        if ent.label_ == \"FACTURE_NUM\":\\n            entities[\"FACTURE_NUMBER\"] = ent.text\\n        elif ent.label_ == \"COMPANY\":\\n            entities[\"COMPANY\"] = ent.text\\n        elif ent.label_ == \"DATE\":\\n            entities[\"DATE\"] = ent.text\\n        elif ent.label_ == \"ADDRESS\":\\n            entities[\"ADDRESS\"] = ent.text\\n        elif ent.label_ == \"QUANTITY\":\\n            entities[\"QUANTITY\"] = int(ent.text)\\n        elif ent.label_ == \"PRICE\":\\n            entities[\"PRICE\"] = float(ent.text)\\n    return entities\\n\\n\\'\\'\\'\\n\\n# Save extracted entities to JSON file\\noutput_file = \"extracted_entities.json\"\\nsave_entities_to_json(entities, output_file)\\nprint(\"Extracted entities saved to:\", output_file)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#I'm gonna create folder fiha annotations data of barcha invoices wntraini 3lih ba3d el model ."
      ],
      "metadata": {
        "id": "7uEIjwxMC_dp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1-\n",
        "config.txt  \n",
        "\n",
        "img_path = /content/Invoice_1.jpg\n",
        "\n",
        "2- annotated_data_folder : fih el annotations\n",
        "\n",
        "3-"
      ],
      "metadata": {
        "id": "Eudxrt2LQrZg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import spacy\n",
        "import json\n",
        "import cv2\n",
        "import pytesseract\n",
        "from spacy.training import Example\n",
        "\n"
      ],
      "metadata": {
        "id": "5Uwdi4CbE6OO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Function to load annotated data from a folder\n",
        "def load_data_from_folder(folder_path):\n",
        "    annotated_data = []\n",
        "    for file_name in os.listdir(folder_path):\n",
        "        if file_name.endswith(\".json\"):\n",
        "            file_path = os.path.join(folder_path, file_name)\n",
        "            with open(file_path, \"r\") as f:\n",
        "                annotated_data.append(json.load(f))\n",
        "    return annotated_data\n",
        "\n",
        "\n",
        "#######################################################\n",
        "\n",
        "# Load annotated data from the folder\n",
        "folder_path = \"annotated_data_folder\"\n",
        "annotated_data = load_data_from_folder(folder_path)\n",
        "\n",
        "# Initialize a blank spaCy model with NER component\n",
        "nlp = spacy.blank(\"en\")\n",
        "ner = nlp.add_pipe(\"ner\")\n",
        "\n",
        "# Add labels to the NER component\n",
        "for annotation in annotated_data:\n",
        "    for label in annotation[\"classes\"]:\n",
        "        ner.add_label(label)\n",
        "\n",
        "# Prepare training data\n",
        "train_data = []\n",
        "for annotation in annotated_data:\n",
        "    for data in annotation[\"annotations\"]:\n",
        "        if data is not None:\n",
        "            text, annotations = data\n",
        "            if annotations:\n",
        "                train_data.append((text, annotations))\n",
        "\n",
        "# Convert annotations to Examples\n",
        "examples = []\n",
        "for text, annotations in train_data:\n",
        "    doc = nlp.make_doc(text)\n",
        "    example = Example.from_dict(doc, annotations)\n",
        "    examples.append(example)\n",
        "\n",
        "# Update the model with the training data\n",
        "optimizer = nlp.begin_training()\n",
        "for _ in range(10):  # Update for 10 iterations\n",
        "    losses = {}\n",
        "    for example in examples:\n",
        "        nlp.update([example], sgd=optimizer, losses=losses)\n",
        "    print(\"Losses:\", losses)\n",
        "\n",
        "# Save the trained model to disk\n",
        "nlp.to_disk(\"trained_ner_model\")\n",
        "\n",
        "\n",
        "#######################################################\n",
        "\n",
        "# Function to read image path from config.txt\n",
        "def read_image_path_from_config(config_file):\n",
        "    with open(config_file, \"r\") as f:\n",
        "        config_data = f.read().strip().split(\"=\")\n",
        "        if len(config_data) == 2 and config_data[0].strip() == \"img_path\":\n",
        "            return config_data[1].strip()\n",
        "        else:\n",
        "            raise ValueError(\"Invalid config.txt format\")\n",
        "\n",
        "# Test the trained model on new invoice images\n",
        "def extract_text_from_image(image_path):\n",
        "    # Read the image using OpenCV\n",
        "    image = cv2.imread(image_path)\n",
        "    # Convert the image to grayscale\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    # Thresholding to remove noise\n",
        "    _, thresholded_image = cv2.threshold(gray_image, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
        "    # Perform OCR using pytesseract\n",
        "    text = pytesseract.image_to_string(thresholded_image, lang='eng', config='--psm 6')\n",
        "    return text\n",
        "\n",
        "def extract_entities_from_text(text):\n",
        "    doc = nlp(text)\n",
        "    entities = {}\n",
        "    for ent in doc.ents:\n",
        "        entities[ent.label_] = ent.text\n",
        "    return entities\n",
        "\n",
        "def save_entities_to_json(entities, output_file):\n",
        "    with open(output_file, \"w\") as json_file:\n",
        "        json.dump(entities, json_file, indent=4)\n",
        "\n",
        "def save_extracted_text_to_file(text, output_file):\n",
        "    with open(output_file, \"w\") as text_file:\n",
        "        text_file.write(text)\n",
        "\n",
        "# Read image path from config.txt\n",
        "config_file = \"config.txt\"\n",
        "image_path = read_image_path_from_config(config_file)\n",
        "\n",
        "# Example usage\n",
        "text = extract_text_from_image(image_path)\n",
        "entities = extract_entities_from_text(text)\n",
        "print(\"Extracted entities:\", entities)\n",
        "\n",
        "# Save extracted entities to JSON file\n",
        "output_file = \"extracted_entities.json\"\n",
        "save_entities_to_json(entities, output_file)\n",
        "print(\"Extracted entities saved to:\", output_file)\n",
        "\n",
        "# Save extracted text to a plain text file\n",
        "extracted_text_file = \"/content/extracted_text.txt\"\n",
        "save_extracted_text_to_file(text, extracted_text_file)\n",
        "print(\"Extracted text saved to:\", extracted_text_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gz62efclP-U_",
        "outputId": "4e8982f3-634b-4096-e22d-308d1bd13194"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Losses: {'ner': 143.01369860146656}\n",
            "Losses: {'ner': 35.70778885065869}\n",
            "Losses: {'ner': 26.535829882151155}\n",
            "Losses: {'ner': 37.19211096883714}\n",
            "Losses: {'ner': 34.73109942744418}\n",
            "Losses: {'ner': 57.547037407443845}\n",
            "Losses: {'ner': 51.835848043511696}\n",
            "Losses: {'ner': 49.92766551074053}\n",
            "Losses: {'ner': 45.9436310959959}\n",
            "Losses: {'ner': 27.47602922141381}\n",
            "Extracted entities: {'FACTURE_NUM': 'if 232.404.4410}\\nhitps:/sellercenter,jumia,com.tninrteriindexindexdinered Status! o\\nScanned with CamScanner\\n\\x0c', 'DESIGNATION': 'Signature et cachet .\\n\\\\\\nSociét? ACTICC\\nham. Bou \\\\ ah Ax |\\nTELLS. “', 'NET': '|| Totalde [litem'}\n",
            "Extracted entities saved to: extracted_entities.json\n",
            "Extracted text saved to: /content/extracted_text.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LOL"
      ],
      "metadata": {
        "id": "r0hIat8CShdP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import pytesseract\n",
        "\n",
        "# Function to read image path from config.txt\n",
        "def read_image_path_from_config(config_file):\n",
        "    with open(config_file, \"r\") as f:\n",
        "        config_data = f.read().strip().split(\"=\")\n",
        "        if len(config_data) == 2 and config_data[0].strip() == \"img_path\":\n",
        "            return config_data[1].strip()\n",
        "        else:\n",
        "            raise ValueError(\"Invalid config.txt format\")\n",
        "\n",
        "# Test the trained model on new invoice images\n",
        "def extract_text_from_image(image_path):\n",
        "    # Read the image using OpenCV\n",
        "    image = cv2.imread(image_path)\n",
        "    # Convert the image to grayscale\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    # Thresholding to remove noise\n",
        "    _, thresholded_image = cv2.threshold(gray_image, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
        "    # Perform OCR using pytesseract\n",
        "    text = pytesseract.image_to_string(thresholded_image, lang='eng', config='--psm 6')\n",
        "    return text\n",
        "\n",
        "# Read image path from config.txt\n",
        "config_file = \"config.txt\"\n",
        "image_path = read_image_path_from_config(config_file)\n",
        "\n",
        "# Example usage\n",
        "text = extract_text_from_image(image_path)\n",
        "\n",
        "# Save extracted text to a plain text file\n",
        "extracted_text_file = \"Only_extracted.txt\"\n",
        "with open(extracted_text_file, \"w\") as text_file:\n",
        "    text_file.write(text)\n",
        "\n",
        "print(\"Extracted text saved to:\", extracted_text_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSvQbSnEg_e_",
        "outputId": "482f742f-34ec-4204-c93e-6327148aaec5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted text saved to: Only_extracted.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "import cv2\n",
        "import pytesseract\n",
        "\n",
        "# Function to read image path from config.txt\n",
        "def read_image_path_from_config(config_file):\n",
        "    with open(config_file, \"r\") as f:\n",
        "        config_data = f.read().strip().split(\"=\")\n",
        "        if len(config_data) == 2 and config_data[0].strip() == \"img_path\":\n",
        "            return config_data[1].strip()\n",
        "        else:\n",
        "            raise ValueError(\"Invalid config.txt format\")\n",
        "\n",
        "# Test the trained model on new invoice images\n",
        "def extract_text_from_image(image_path):\n",
        "    # Read the image using OpenCV\n",
        "    image = cv2.imread(image_path)\n",
        "    # Convert the image to grayscale\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    # Apply Gaussian blur to reduce noise and improve OCR accuracy\n",
        "    blurred_image = cv2.GaussianBlur(gray_image, (5, 5), 0)\n",
        "    # Adaptive thresholding to binarize the image\n",
        "    thresholded_image = cv2.adaptiveThreshold(blurred_image, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 11, 2)\n",
        "    # Invert colors\n",
        "    inverted_image = cv2.bitwise_not(thresholded_image)\n",
        "    # Perform OCR using pytesseract\n",
        "    text = pytesseract.image_to_string(inverted_image, lang='eng', config='--psm 6')\n",
        "    return text\n",
        "\n",
        "# Read image path from config.txt\n",
        "config_file = \"config.txt\"\n",
        "image_path = read_image_path_from_config(config_file)\n",
        "\n",
        "# Example usage\n",
        "text = extract_text_from_image(image_path)\n",
        "\n",
        "# Save extracted text to a plain text file\n",
        "extracted_text_file = \"extracted.txt\"\n",
        "with open(extracted_text_file, \"w\") as text_file:\n",
        "    text_file.write(text)\n",
        "\n",
        "print(\"Extracted text saved to:\", extracted_text_file)\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "907_t_LQp5ZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!sudo apt-get install tesseract-ocr\n",
        "!export PATH=\"/usr/bin:$PATH\"\n",
        "\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "# If Tesseract is not automatically found in the system path, you can specify its location\n",
        "# Replace the path below with the actual path where Tesseract is installed in your Colab environment\n",
        "pytesseract.pytesseract.tesseract_cmd = '/usr/bin/tesseract'\n",
        "\n",
        "\n",
        "# Path to the image file\n",
        "image_path = \"/content/Invoice_1.jpg\"\n",
        "\n",
        "# Open the image using PIL (Python Imaging Library)\n",
        "img = Image.open(image_path)\n",
        "\n",
        "# Use Tesseract to do OCR on the image\n",
        "text = pytesseract.image_to_string(img)\n",
        "\n",
        "# Print the extracted text\n",
        "print(\"Extracted Text:\")\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QK5HuUE_rTCQ",
        "outputId": "9874a6e6-ad5b-4968-8e17-ffebd310ab0b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted Text:\n",
            "25/12/2023 13:29 Gérer les Commandes | Ready To Ship | Seller Center\n",
            "\n",
            "FACTURE UOMO\n",
            "So) Facturé a:\n",
            "\n",
            " \n",
            "\n",
            "Société SIRAT\n",
            "\n",
            "Expéditeur:DIDACTICO Addresse:\n",
            "Adresse: Société SIRAT\n",
            "Matricule fiscal:1271640/RPM000 immeuble 10 rue Brésil Tunis\n",
            "Date :25 Dec 2023 Belvédére 1002\n",
            "Commande numéro:308144111 Belvedére\n",
            "\n",
            "Tunis\n",
            "\n",
            "Tunisia\n",
            "\n",
            "Compagnie de ;\n",
            "livraison: TN-ALLO-COLIS\n",
            "TUNIS-Fleet\n",
            "\n",
            "PRIX UNITAIRE ||\n",
            "\n",
            "PRIX UNITAIRE Totalde [item\n",
            "RESIGNATION fauanrrrel HT | Tc réduction _ [variation\n",
            "\n",
            "Adaptateur HDMI vers VGAl| ade tod too WA\n",
            "- Noir iE :\n",
            "\n",
            "rais de livraison 4\n",
            "otal HT\n",
            "\n",
            "   \n",
            "\n",
            "   \n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            " \n",
            " \n",
            " \n",
            "\n",
            " \n",
            " \n",
            "\n",
            "   \n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "    \n",
            " \n",
            "\n",
            " \n",
            "\n",
            "   \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "9%\n",
            "‘otal TTC 7,75\n",
            "otal a collecter 17,75 TND\n",
            "\n",
            " \n",
            "\n",
            "Signature et cachet\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "hitps//sellercenter jumi,com.tninrerfindexlindettiteredStatus/®\n",
            "\n",
            "Scanned with CamScanner\n",
            "\f\n"
          ]
        }
      ]
    }
  ]
}